# -*- coding: utf-8 -*-
"""Sentistream_140_Dashboard.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D_dCXs0gqo_oLcVWxVMR0U7pFwGp2MpR

# 1. ë°ì´í„° ë¡œë“œ
"""

# 1) Kaggle API í† í° ì—…ë¡œë“œ
from google.colab import files
print("âš ï¸ kaggle.json íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
files.upload()  # ë¡œì»¬ì˜ kaggle.json íŒŒì¼ ì„ íƒ

# 2) kaggle ì„¤ì • ë””ë ‰í† ë¦¬ ìƒì„± ë° ê¶Œí•œ ë¶€ì—¬
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# 3) Kaggle íŒ¨í‚¤ì§€ ì„¤ì¹˜
!pip install --upgrade kaggle

# 4) Sentiment140 ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ
!kaggle datasets download -d kazanova/sentiment140 -p /content
!unzip -q /content/sentiment140.zip -d /content/sentiment140

# 5) pandasë¡œ ë°ì´í„°ì…‹ ë¡œë“œ
import pandas as pd

df = pd.read_csv(
    '/content/sentiment140/training.1600000.processed.noemoticon.csv',
    encoding='latin-1',
    names=['target', 'ids', 'date', 'flag', 'user', 'text']
)

# ë¼ë²¨ ë§¤í•‘
df['sentiment'] = df['target'].map({0: 'negative', 4: 'positive'})
# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
df = df[['text', 'sentiment']]

# ë°ì´í„° í™•ì¸
df.head()

"""# 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì‘ì„±"""

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì´ë¯¸ ì„¤ì¹˜ëœ ê²½ìš° ìƒëµ)
!pip install --quiet nltk emoji

# NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (punkt ì œê±°)
import nltk
nltk.download('stopwords')
nltk.download('wordnet')

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import re
import emoji
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# ë¶ˆìš©ì–´ ë° í‘œì œì–´ ë„êµ¬ ì´ˆê¸°í™”
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    """
    punkt ì˜ì¡´ ì—†ì´:
    - ì†Œë¬¸ì ë³€í™˜
    - URL, ì´ëª¨ì§€ ì œê±°
    - íŠ¹ìˆ˜ë¬¸ì ì œê±°
    - ê³µë°± ê¸°ì¤€ í† í°í™”
    - ë¶ˆìš©ì–´ ì œê±° ë° í‘œì œì–´ ì¶”ì¶œ
    """
    text = text.lower()
    text = re.sub(r'http\S+|www\.\S+', '', text)
    text = emoji.replace_emoji(text, replace='')
    text = re.sub(r'[^a-z\s]', ' ', text)

    tokens = text.split()
    processed_tokens = [
        lemmatizer.lemmatize(token)
        for token in tokens
        if token not in stop_words and len(token) > 1
    ]

    return ' '.join(processed_tokens)

# ì˜ˆì‹œ í…ŒìŠ¤íŠ¸
example = "I love NLP! Visit https://example.com ğŸ˜Š #AI #NLP"
print("Original:", example)
print("Processed:", preprocess(example))

"""# 3. ë²¡í„°í™”"""

# Colab í™˜ê²½ì—ì„œ TF-IDF ë²¡í„°í™”

from sklearn.feature_extraction.text import TfidfVectorizer

# ì´ì „ ë‹¨ê³„ì—ì„œ dfì™€ preprocess í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •
try:
    df
    preprocess
except NameError:
    print("âš ï¸ ì˜¤ë¥˜: df ë˜ëŠ” preprocessê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € load_dataì™€ preprocess ë‹¨ê³„ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
else:
    # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ ìƒì„±
    if 'processed' not in df.columns:
        df['processed'] = df['text'].apply(preprocess)

    # TF-IDF ë²¡í„°ë¼ì´ì € ì„¤ì • ë° ë³€í™˜
    vectorizer = TfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 2)
    )
    X_tfidf = vectorizer.fit_transform(df['processed'])

    # ê²°ê³¼ í™•ì¸
    print(f"TF-IDF í–‰ë ¬ í¬ê¸°: {X_tfidf.shape}")

"""# 4. ëª¨ë¸ í•™ìŠµ"""

# Colab í™˜ê²½ì—ì„œ ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ë¥˜ê¸° í•™ìŠµ (GridSearchCV)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# ì´ì „ ë‹¨ê³„ì—ì„œ X_tfidfì™€ dfê°€ ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •
try:
    X_tfidf
    df
except NameError:
    print("âš ï¸ ì˜¤ë¥˜: X_tfidf ë˜ëŠ” dfê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € vectorize ë‹¨ê³„ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
else:
    # ë ˆì´ë¸” ì¸ì½”ë”©: negative=0, positive=1
    y = df['sentiment'].map({'negative': 0, 'positive': 1})

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •
    param_grid = {'C': [0.01, 0.1, 1, 10]}

    # ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ë° GridSearchCV ì„¤ì •
    lr = LogisticRegression(max_iter=1000, solver='liblinear')
    grid_search = GridSearchCV(
        estimator=lr,
        param_grid=param_grid,
        cv=5,
        scoring='f1',
        n_jobs=-1,
        verbose=1
    )

    # ëª¨ë¸ í•™ìŠµ
    grid_search.fit(X_tfidf, y)

    # ìµœì  ê²°ê³¼ ì¶œë ¥
    print("Best C:", grid_search.best_params_['C'])
    print("Best F1 Score:", grid_search.best_score_)

    # ìµœì  ëª¨ë¸ ì €ì¥
    best_model = grid_search.best_estimator_

"""# 5. í‰ê°€ ë° ì‹œê°í™”"""

# Colab í™˜ê²½ì—ì„œ ëª¨ë¸ í‰ê°€: í˜¼ëˆí–‰ë ¬, classification_report, ROC ì»¤ë¸Œ

from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt

# ì´ì „ ë‹¨ê³„ì—ì„œ best_model, X_tfidf, dfê°€ ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •
try:
    best_model
    X_tfidf
    df
except NameError:
    print("âš ï¸ ì˜¤ë¥˜: best_model, X_tfidf ë˜ëŠ” dfê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train_model ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
else:
    # ë ˆì´ë¸” ì¸ì½”ë”©
    y_true = df['sentiment'].map({'negative': 0, 'positive': 1})

    # ì˜ˆì¸¡ ë° ì˜ˆì¸¡ í™•ë¥ 
    y_pred = best_model.predict(X_tfidf)
    y_prob = best_model.predict_proba(X_tfidf)[:, 1]

    # í˜¼ëˆí–‰ë ¬ ì¶œë ¥
    cm = confusion_matrix(y_true, y_pred)
    print("Confusion Matrix:")
    print(cm)
    print()

    # ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì¶œë ¥
    report = classification_report(y_true, y_pred, target_names=['negative', 'positive'])
    print("Classification Report:")
    print(report)

    # ROC ì»¤ë¸Œ ê³„ì‚°
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)

    # ROC ì»¤ë¸Œ ì‹œê°í™”
    plt.figure()
    plt.plot(fpr, tpr)
    plt.plot([0, 1], [0, 1], linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve (AUC = {roc_auc:.2f})')
    plt.show()

import pandas as pd
import pickle

# 1) í›ˆë ¨ ë°ì´í„° ì €ì¥
df.to_csv('df.csv', index=False)

# 2) ë²¡í„°ë¼ì´ì € ì €ì¥
with open('vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer, f)

# 3) ìµœì  ëª¨ë¸ ì €ì¥
with open('model.pkl', 'wb') as f:
    pickle.dump(best_model, f)

print("âœ… df.csv, vectorizer.pkl, model.pkl íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat > app.py << 'EOF'
# import streamlit as st
# import pandas as pd
# import pickle
# import re
# import nltk
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import (
#     confusion_matrix, roc_curve, auc, precision_score, recall_score
# )
# import plotly.express as px
# from wordcloud import WordCloud
# 
# # 1. NLTK ì„¸íŒ… (ìµœì´ˆ 1íšŒ)
# nltk.download('stopwords')
# nltk.download('wordnet')
# 
# # 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ (ê°„ë‹¨ ê³µë°± í† í°í™”)
# stop_words = set(stopwords.words('english'))
# lemmatizer = WordNetLemmatizer()
# def preprocess(text):
#     text = text.lower()
#     text = re.sub(r'http\\S+|www\\.\\S+', '', text)
#     text = re.sub(r'[^a-z\\s]', ' ', text)
#     tokens = text.split()
#     return ' '.join(lemmatizer.lemmatize(t) for t in tokens
#                     if t not in stop_words and len(t)>1)
# 
# # 3. ë°ì´í„°Â·ëª¨ë¸ ë¡œë“œ
# df = pd.read_csv('df.csv')
# vectorizer = pickle.load(open('vectorizer.pkl','rb'))
# default_model = pickle.load(open('model.pkl','rb'))
# 
# # 4. ì‚¬ì´ë“œë°” ì»¨íŠ¸ë¡¤
# st.sidebar.header("âš™ï¸ Settings")
# 
# # 4.1 ì„ê³„ê°’ ìŠ¬ë¼ì´ë”
# threshold = st.sidebar.slider(
#     "Positive Threshold",
#     min_value=0.0, max_value=1.0, value=0.5, step=0.01
# )
# 
# # 4.2 í•˜ì´í¼íŒŒë¼ë¯¸í„° C ì¡°ì •
# C_val = st.sidebar.select_slider(
#     "LogReg C (regularization)",
#     options=[0.01, 0.1, 1.0, 10.0], value=1.0
# )
# 
# # 5. ëª¨ë¸ ì¬í•™ìŠµ (C ë³€ê²½ ì‹œ)
# @st.cache(allow_output_mutation=True, show_spinner=False)
# def train(C):
#     X = vectorizer.transform(df['text'].apply(preprocess))
#     y = df['sentiment'].map({'negative':0,'positive':1})
#     lr = LogisticRegression(C=C, max_iter=1000, solver='liblinear')
#     lr.fit(X, y)
#     return lr
# 
# model = train(C_val)
# 
# # 6. ì „ì²´ ë°ì´í„° í‰ê°€ ì§€í‘œ
# X_all = vectorizer.transform(df['text'].apply(preprocess))
# y_true = df['sentiment'].map({'negative':0,'positive':1})
# y_prob = model.predict_proba(X_all)[:,1]
# y_pred = (y_prob >= threshold).astype(int)
# 
# acc = (y_pred==y_true).mean()
# prec = precision_score(y_true, y_pred)
# rec = recall_score(y_true, y_pred)
# 
# st.set_page_config(layout="wide")
# st.title("ğŸ¯ Batch Sentiment Analysis Dashboard")
# 
# col1, col2, col3 = st.columns(3)
# col1.metric("Accuracy", f"{acc:.3f}")
# col2.metric("Precision", f"{prec:.3f}")
# col3.metric("Recall", f"{rec:.3f}")
# 
# # 7. í˜¼ëˆí–‰ë ¬ & ROC (Plotly)
# cm = confusion_matrix(y_true, y_pred)
# cm_df = pd.DataFrame(cm, index=['Actual Neg','Actual Pos'], columns=['Pred Neg','Pred Pos'])
# fig_cm = px.imshow(
#     cm_df, text_auto=True, color_continuous_scale='Blues',
#     labels={'x':'Predicted','y':'Actual'}, title="Confusion Matrix"
# )
# fpr, tpr, _ = roc_curve(y_true, y_prob)
# roc_auc = auc(fpr, tpr)
# fig_roc = px.area(
#     x=fpr, y=tpr,
#     title=f"ROC Curve (AUC={roc_auc:.2f})",
#     labels={'x':'False Positive Rate','y':'True Positive Rate'},
#     line_shape='linear'
# )
# fig_roc.add_shape(
#     type='line', line=dict(dash='dash'),
#     x0=0, x1=1, y0=0, y1=1
# )
# 
# st.plotly_chart(fig_cm, use_container_width=True)
# st.plotly_chart(fig_roc, use_container_width=True)
# 
# # 8. ìƒìœ„ ë‹¨ì–´ ë§‰ëŒ€ê·¸ë˜í”„
# def top_ngrams(sentiment, n=20):
#     corpus = df[df['sentiment']==sentiment]['text'].apply(preprocess)
#     vec = TfidfVectorizer(max_features=5000, ngram_range=(1,1))
#     X = vec.fit_transform(corpus)
#     sums = X.sum(axis=0).A1
#     terms = vec.get_feature_names_out()
#     top = sorted(zip(terms, sums), key=lambda x: x[1], reverse=True)[:n]
#     return pd.DataFrame(top, columns=['term','tfidf'])
# 
# bar_neg = top_ngrams('negative')
# bar_pos = top_ngrams('positive')
# 
# fig_bar_pos = px.bar(bar_pos, x='term', y='tfidf',
#                      title="Top Positive Words", labels={'tfidf':'TF-IDF'})
# fig_bar_neg = px.bar(bar_neg, x='term', y='tfidf',
#                      title="Top Negative Words", labels={'tfidf':'TF-IDF'})
# 
# st.plotly_chart(fig_bar_pos, use_container_width=True)
# st.plotly_chart(fig_bar_neg, use_container_width=True)
# 
# # 9. ì›Œë“œí´ë¼ìš°ë“œ (í¬ê¸°/ë ˆì´ì•„ì›ƒ ì¡°ì •)
# wc_pos = WordCloud(
#     width=600, height=300, background_color='white'
# ).generate(' '.join(df[df['sentiment']=='positive']['text'].apply(preprocess)))
# wc_neg = WordCloud(
#     width=600, height=300, background_color='white'
# ).generate(' '.join(df[df['sentiment']=='negative']['text'].apply(preprocess)))
# 
# st.subheader("Positive Word Cloud")
# st.image(wc_pos.to_array())
# 
# st.subheader("Negative Word Cloud")
# st.image(wc_neg.to_array())
# EOF

"""# 6. Streamlit"""

!pip install --quiet pyngrok
!ngrok authtoken 2yzg6wRvAmJyCgSnoEQUjJMRKtA_3rVWf7k79ubJKKgRr8uxZ

# 1) streamlit ì„¤ì¹˜ (í•„ìš” ì‹œ)
!pip install --quiet streamlit

# 2) Streamlitì„ 0.0.0.0:8501ì— ë°”ì¸ë”©í•˜ê³  ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
import os
os.system(
    "nohup streamlit run app.py "
    "--server.port=8501 "
    "--server.address=0.0.0.0 "
    "--server.headless=true > st.log 2>&1 &"
)

from pyngrok import ngrok

# í˜¹ì‹œ ì—´ë¦° í„°ë„ì´ ìˆìœ¼ë©´ ë‹«ê³ 
ngrok.kill()

# ì´ì œ 0.0.0.0:8501 ë¡œ ë°”ì¸ë”©ëœ Streamlitì„ í„°ë„ë§
public_url = ngrok.connect(addr="8501", proto="http")
print("ğŸ”— Public URL:", public_url)