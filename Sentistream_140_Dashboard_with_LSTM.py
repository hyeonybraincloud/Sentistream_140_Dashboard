# -*- coding: utf-8 -*-
"""NLP_something_advanced.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r1hVGAbdUPZfe6SCnOJX4YyrAZ2gisWh

# 1. ë°ì´í„° ë¡œë“œ
"""

# 1) Kaggle API í† í° ì—…ë¡œë“œ
from google.colab import files
print("âš ï¸ kaggle.json íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
files.upload()  # ë¡œì»¬ì˜ kaggle.json íŒŒì¼ ì„ íƒ

# 2) kaggle ì„¤ì • ë””ë ‰í† ë¦¬ ìƒì„± ë° ê¶Œí•œ ë¶€ì—¬
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# 3) Kaggle íŒ¨í‚¤ì§€ ì„¤ì¹˜
!pip install --upgrade kaggle

# 4) Sentiment140 ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ
!kaggle datasets download -d kazanova/sentiment140 -p /content
!unzip -q /content/sentiment140.zip -d /content/sentiment140

# 5) pandasë¡œ ë°ì´í„°ì…‹ ë¡œë“œ
import pandas as pd

df = pd.read_csv(
    '/content/sentiment140/training.1600000.processed.noemoticon.csv',
    encoding='latin-1',
    names=['target', 'ids', 'date', 'flag', 'user', 'text']
)

# ë¼ë²¨ ë§¤í•‘
df['sentiment'] = df['target'].map({0: 'negative', 4: 'positive'})
# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
df = df[['text', 'sentiment']]

# ë°ì´í„° í™•ì¸
df.head()

"""# 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì‘ì„±"""

# Colab í™˜ê²½ìš© ìˆ˜ì •ëœ preprocess í•¨ìˆ˜ (punkt ì˜ì¡´ ì œê±°)

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì´ë¯¸ ì„¤ì¹˜ëœ ê²½ìš° ìƒëµ)
!pip install --quiet nltk emoji

# NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (punkt ì œê±°)
import nltk
nltk.download('stopwords')
nltk.download('wordnet')

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import re
import emoji
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# ë¶ˆìš©ì–´ ë° í‘œì œì–´ ë„êµ¬ ì´ˆê¸°í™”
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    """
    punkt ì˜ì¡´ ì—†ì´:
    - ì†Œë¬¸ì ë³€í™˜
    - URL, ì´ëª¨ì§€ ì œê±°
    - íŠ¹ìˆ˜ë¬¸ì ì œê±°
    - ê³µë°± ê¸°ì¤€ í† í°í™”
    - ë¶ˆìš©ì–´ ì œê±° ë° í‘œì œì–´ ì¶”ì¶œ
    """
    text = text.lower()
    text = re.sub(r'http\S+|www\.\S+', '', text)
    text = emoji.replace_emoji(text, replace='')
    text = re.sub(r'[^a-z\s]', ' ', text)

    tokens = text.split()
    processed_tokens = [
        lemmatizer.lemmatize(token)
        for token in tokens
        if token not in stop_words and len(token) > 1
    ]

    return ' '.join(processed_tokens)

# ì˜ˆì‹œ í…ŒìŠ¤íŠ¸
example = "I love NLP! Visit https://example.com ğŸ˜Š #AI #NLP"
print("Original:", example)
print("Processed:", preprocess(example))

"""# 3. Embedding ê¸°ë°˜ í† í°í™” ë° ì‹œí€€ìŠ¤ ìƒì„±"""

from sklearn.feature_extraction.text import TfidfVectorizer

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense


# ì´ì „ ë‹¨ê³„ì—ì„œ dfì™€ preprocess í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •
try:
    df
    preprocess
except NameError:
    print("âš ï¸ ì˜¤ë¥˜: df ë˜ëŠ” preprocessê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € load_dataì™€ preprocess ë‹¨ê³„ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
else:
    # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ ìƒì„±
    if 'processed' not in df.columns:
        df['processed'] = df['text'].apply(preprocess)

    # --- 3. Embedding ê¸°ë°˜ í† í°í™” & ì‹œí€€ìŠ¤ ìƒì„± ---
    # 3-1) Tokenizer ì„¤ì • (ìƒìœ„ 5,000ê°œ ë‹¨ì–´ë§Œ)
    tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
    tokenizer.fit_on_texts(df['processed'])
    sequences = tokenizer.texts_to_sequences(df['processed'])

    # 3-2) íŒ¨ë”© (ìµœëŒ€ ê¸¸ì´ 100)
    X_seq = pad_sequences(sequences, maxlen=100, padding='post')

    # 3-3) ë ˆì´ë¸” ì´ì§„ ì¸ì½”ë”© (ì˜ˆ: 'negative'/'positive' â†’ 0/1)
    y = df['sentiment'].map({'negative':0, 'positive':1}).values  # ì»¬ëŸ¼ëª…Â·ë§¤í•‘ì€ ë°ì´í„°í”„ë ˆì„ì— ë§ê²Œ ì¡°ì •

"""# 4. LSTM ë¶„ë¥˜ê¸° ì •ì˜ ë° í•™ìŠµ"""

# --- 4. LSTM ë¶„ë¥˜ê¸° ì •ì˜ & í•™ìŠµ ---
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Bidirectional

model = Sequential([
    Embedding(5000, 128, input_length=100),
    Bidirectional(LSTM(64, return_sequences=False)),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer=Adam(learning_rate=5e-4),  # í•™ìŠµë¥  ì ˆë°˜ìœ¼ë¡œ ë‚®ì¶¤
    loss='binary_crossentropy',
    metrics=['accuracy']
)

X_train, X_val, y_train, y_val = train_test_split(
    X_seq, y, test_size=0.2, stratify=y, random_state=42
)

# í•™ìŠµ
history = model.fit(
    X_train, y_train,
    epochs=10,               # ì—í­ ìˆ˜ ëŠ˜ë ¤ì„œ ì‹¤í—˜
    batch_size=64,
    validation_data=(X_val, y_val)
)

# â–¼ ì—¬ê¸°ì„œë¶€í„° í† í¬ë‚˜ì´ì € ì €ì¥ â–¼
tokenizer_json = tokenizer.to_json()
with open('tokenizer.json','w', encoding='utf-8') as f:
    f.write(tokenizer_json)

"""# 5. LSTM ëª¨ë¸ í‰ê°€ ë° ì €ì¥"""

# 5. LSTM ëª¨ë¸ í‰ê°€ ë° ì¶”ê°€ ì§€í‘œ ê³„ì‚°(sklearn)
from sklearn.metrics import (
    accuracy_score,
    log_loss,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    classification_report,
    confusion_matrix
)

# 5-1) ì˜ˆì¸¡ í™•ë¥  ë° ì´ì§„ ì˜ˆì¸¡ê°’ ìƒì„±
y_prob = model.predict(X_seq).ravel()
y_pred = (y_prob >= 0.5).astype(int)

# 5-2) ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
loss     = log_loss(y, y_prob)
accuracy = accuracy_score(y, y_pred)
prec   = precision_score(y, y_pred)
rec    = recall_score(y, y_pred)
f1     = f1_score(y, y_pred)
auc    = roc_auc_score(y, y_prob)
cm     = confusion_matrix(y, y_pred)
report = classification_report(y, y_pred)

# 5-3) ê²°ê³¼ ì¶œë ¥
print(f"LSTM ëª¨ë¸ â€” loss (log_loss): {loss:.4f}, accuracy: {accuracy:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1-score:  {f1:.4f}")
print(f"AUC-ROC:   {auc:.4f}")
print("Confusion Matrix:\n", cm)
print("\nClassification Report:\n", report)

"""# 6. LSTM ëª¨ë¸ ì €ì¥"""

# 6. LSTM ëª¨ë¸ ì €ì¥
model.save('lstm_model.h5')
print("âœ… LSTM ëª¨ë¸ì„ 'lstm_model.h5'ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat > app.py << 'EOF'
# import streamlit as st
# import json
# import re
# import emoji
# from tensorflow.keras.models import load_model
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# from tensorflow.keras.preprocessing.text import tokenizer_from_json
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer
# 
# # 1) ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
# model = load_model('lstm_model.h5')
# with open('tokenizer.json','r', encoding='utf-8') as f:
#     tokenizer = tokenizer_from_json(f.read())
# 
# # 2) ì „ì²˜ë¦¬ í•¨ìˆ˜ (í•™ìŠµ ë•Œì™€ ë™ì¼í•˜ê²Œ ë³µì‚¬)
# stop_words = set(stopwords.words('english'))
# lemmatizer = WordNetLemmatizer()
# def preprocess(text):
#     text = text.lower()
#     text = re.sub(r'http\\S+|www\\.\\S+', '', text)
#     text = emoji.replace_emoji(text, replace='')
#     text = re.sub(r'[^a-z\\s]', ' ', text)
#     tokens = text.split()
#     return ' '.join(lemmatizer.lemmatize(t) for t in tokens
#                     if t not in stop_words and len(t)>1)
# 
# # 3) Streamlit UI
# st.title("ğŸ¯ LSTM Sentiment Analysis")
# user_input = st.text_area("Enter text to analyze:")
# if st.button("Predict"):
#     processed = preprocess(user_input)
#     seq = tokenizer.texts_to_sequences([processed])
#     padded = pad_sequences(seq, maxlen=100, padding='post')
#     prob = model.predict(padded)[0,0]
#     st.write(f"Positive probability: {prob:.3f}")
# EOF

!pip install --quiet pyngrok
!ngrok authtoken (secret)

# 1) streamlit ì„¤ì¹˜ (í•„ìš” ì‹œ)
!pip install --quiet streamlit

# 2) Streamlitì„ 0.0.0.0:8501ì— ë°”ì¸ë”©í•˜ê³  ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
import os
os.system(
    "nohup streamlit run app.py "
    "--server.port=8501 "
    "--server.address=0.0.0.0 "
    "--server.headless=true > st.log 2>&1 &"
)

from pyngrok import ngrok

# í˜¹ì‹œ ì—´ë¦° í„°ë„ì´ ìˆìœ¼ë©´ ë‹«ê³ 
ngrok.kill()

# ì´ì œ 0.0.0.0:8501 ë¡œ ë°”ì¸ë”©ëœ Streamlitì„ í„°ë„ë§
public_url = ngrok.connect(addr="8501", proto="http")
print("ğŸ”— Public URL:", public_url)
